{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy in Anaconda Command prompt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm in Anaconda Command prompt\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba PROPN poss Alibaba NNP Xxxxx True False\n",
      "'s PART case 's POS 'x False True\n",
      "Ant PROPN nmod Ant NNP Xxx True False\n",
      "Financial PROPN npadvmod Financial NNP Xxxxx True False\n",
      "- PUNCT punct - HYPH - False False\n",
      "backed VERB amod back VBN xxxx True False\n",
      "online ADJ amod online JJ xxxx True False\n",
      "food NOUN compound food NN xxxx True False\n",
      "delivery NOUN nsubj delivery NN xxxx True False\n",
      "and CCONJ cc and CC xxx True True\n",
      "restaurant NOUN compound restaurant NN xxxx True False\n",
      "discovery NOUN compound discovery NN xxxx True False\n",
      "platform NOUN compound platform NN xxxx True False\n",
      "Zomato PROPN conj Zomato NNP Xxxxx True False\n",
      "has AUX aux have VBZ xxx True True\n",
      "acquired VERB ROOT acquire VBN xxxx True False\n",
      "Uber PROPN compound Uber NNP Xxxx True False\n",
      "Eats PROPN dobj Eats NNP Xxxx True False\n",
      ", PUNCT punct , , , False False\n",
      "the DET det the DT xxx True True\n",
      "food NOUN compound food NN xxxx True False\n",
      "delivery NOUN compound delivery NN xxxx True False\n",
      "business NOUN appos business NN xxxx True False\n",
      "of ADP prep of IN xx True True\n",
      "ride NOUN npadvmod ride NN xxxx True False\n",
      "- PUNCT punct - HYPH - False False\n",
      "hailing VERB amod hail VBG xxxx True False\n",
      "giant NOUN compound giant NN xxxx True False\n",
      "Uber PROPN compound Uber NNP Xxxx True False\n",
      "India PROPN pobj India NNP Xxxxx True False\n",
      "for ADP prep for IN xxx True True\n",
      "around ADV advmod around RB xxxx True True\n",
      "Rs NOUN nmod r NNS Xx True False\n",
      "2,485 NUM nummod 2,485 CD d,ddd False False\n",
      "crore NOUN pobj crore NN xxxx True False\n",
      "( PUNCT punct ( -LRB- ( False False\n",
      "$ SYM quantmod $ $ $ False False\n",
      "350 NUM compound 350 CD ddd False False\n",
      "million NUM appos million CD xxxx True False\n",
      ") PUNCT punct ) -RRB- ) False False\n",
      "in ADP prep in IN xx True True\n",
      "an DET det an DT xx True True\n",
      "all DET det all DT xxx True True\n",
      "- PUNCT punct - HYPH - False False\n",
      "stock NOUN compound stock NN xxxx True False\n",
      "deal NOUN pobj deal NN xxxx True False\n",
      ". PUNCT punct . . . False False\n"
     ]
    }
   ],
   "source": [
    "para = nlp(\"Alibaba's Ant Financial-backed online food delivery and restaurant discovery platform Zomato has acquired Uber Eats, the food delivery business of ride-hailing giant Uber India for around Rs 2,485 crore ($350 million) in an all-stock deal.\")\n",
    "for words in para:\n",
    "    print(words.text, words.pos_, words.dep_, words.lemma_, words.tag_, words.shape_, words.is_alpha, words.is_stop)\n",
    "    #text is the original text of the lexeme\n",
    "    #pos is the simple part-of-speech tag\n",
    "    #dep is the syntactic dependency i.e. the relation between tokens\n",
    "    #lemma is the base form of the word\n",
    "    #tag is the detailed part-of-speech tag\n",
    "    #shape is the word shape - capitalization, punctuation, digits\n",
    "    #is_alpha is whether the token is an alpha character\n",
    "    #is_stop is whether the token is part of a stop list i.e. the most common words of the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba 0 7 ORG\n",
      "Ant Financial 10 23 ORG\n",
      "Zomato 86 92 GPE\n",
      "Uber Eats 106 115 ORG\n",
      "Uber India 166 176 ORG\n",
      "2,485 191 196 CARDINAL\n",
      "$350 million 204 216 MONEY\n"
     ]
    }
   ],
   "source": [
    "for entities in para.ents:\n",
    "    print(entities.text, entities.start_char, entities.end_char, entities.label_)\n",
    "    #text is the original entity text\n",
    "    #start_char is the index of start of entity\n",
    "    #end_char is the index of end of entity\n",
    "    #label is entity label i.e. type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba True 21.996586 True\n",
      "'s True 21.219128 True\n",
      "Ant True 21.123112 True\n",
      "Financial True 19.393559 True\n",
      "- True 23.283134 True\n",
      "backed True 21.71244 True\n",
      "online True 17.555346 True\n",
      "food True 20.328575 True\n",
      "delivery True 19.069447 True\n",
      "and True 22.130964 True\n",
      "restaurant True 18.034246 True\n",
      "discovery True 18.531908 True\n",
      "platform True 17.778807 True\n",
      "Zomato True 22.53563 True\n",
      "has True 22.474064 True\n",
      "acquired True 21.299208 True\n",
      "Uber True 20.857506 True\n",
      "Eats True 21.80761 True\n",
      ", True 21.101221 True\n",
      "the True 20.774448 True\n",
      "food True 19.287928 True\n",
      "delivery True 18.139269 True\n",
      "business True 18.707468 True\n",
      "of True 24.315151 True\n",
      "ride True 19.188444 True\n",
      "- True 22.68037 True\n",
      "hailing True 21.382889 True\n",
      "giant True 19.886787 True\n",
      "Uber True 21.241734 True\n",
      "India True 21.995556 True\n",
      "for True 20.265371 True\n",
      "around True 20.628675 True\n",
      "Rs True 17.769388 True\n",
      "2,485 True 21.765858 True\n",
      "crore True 21.0941 True\n",
      "( True 20.21262 True\n",
      "$ True 22.927814 True\n",
      "350 True 23.755072 True\n",
      "million True 23.420534 True\n",
      ") True 22.008068 True\n",
      "in True 20.747879 True\n",
      "an True 19.479261 True\n",
      "all True 18.76683 True\n",
      "- True 22.663694 True\n",
      "stock True 19.706097 True\n",
      "deal True 19.616573 True\n",
      ". True 21.21185 True\n"
     ]
    }
   ],
   "source": [
    "for words in para:\n",
    "    print(words.text, words.has_vector, words.vector_norm, words.is_oov)\n",
    "    #has_vector is whether the token has a vector representation \n",
    "    #vector_norm is the L2 norm of the token’s vector (the square root of the sum of the values squared) \n",
    "    #is_oov is Out-of-vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5983206397430101660\n"
     ]
    }
   ],
   "source": [
    "print(para.vocab.strings[\"unique\"])\n",
    "#gives the id for a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject True 5.62777 False\n",
      "word True 5.8387117 False\n",
      "intelligence True 7.082624 False\n",
      "lalalala True 6.225437 False\n"
     ]
    }
   ],
   "source": [
    "#python -m spacy download en_core_web_md in Anaconda Command prompt\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "tokens = nlp(\"subject word intelligence lalalala\")\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject subject 1.0\n",
      "subject word 0.35317358\n",
      "subject intelligence 0.2781774\n",
      "word subject 0.35317358\n",
      "word word 1.0\n",
      "word intelligence 0.30542892\n",
      "intelligence subject 0.2781774\n",
      "intelligence word 0.30542892\n",
      "intelligence intelligence 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"subject word intelligence\")\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "        #similarity - returns a float value that a scalar similarity score. Higher is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, world. Here are two sentences.\")\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser']\n",
      "<spacy.lang.en.English object at 0x00000142BA61B888>\n"
     ]
    }
   ],
   "source": [
    "#Training and updating neural network models\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "train_data = [(\"Alibaba's Ant Financial-backed online food delivery and restaurant discovery platform Zomato has acquired Uber Eats, the food delivery business of ride-hailing giant Uber India for around Rs 2,485 crore ($350 million) in an all-stock deal.\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"] #get names of other pipes to disable them during training\n",
    "print(other_pipes)\n",
    "with nlp.disable_pipes(*other_pipes): #only train NER\n",
    "    optimizer = nlp.begin_training() #reset and initialize the weights randomly – but only if we're training a new model\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data) \n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer) #update the model\n",
    "nlp.to_disk(\"/model\") # save the trained model\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hi\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '\\\\customer_feedback_1.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-4e00f0ff9338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomer_feedback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/customer_feedback_1.bin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#new_doc = Doc(Vocab()).from_disk(\"/customer_feedback_1.bin\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.to_disk\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[1;32m-> 1203\u001b[1;33m                        opener=self._opener)\n\u001b[0m\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1056\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         \u001b[1;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1058\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '\\\\customer_feedback_1.bin'"
     ]
    }
   ],
   "source": [
    "#serialization\n",
    "#If you’ve been modifying the pipeline, vocabulary, vectors and entities, or made updates to the model, you’ll eventually want to save your progress\n",
    "#This means you’ll have to translate its contents and structure into a format that can be saved, like a file or a byte string. This process is called serialization. spaCy comes with built-in serialization methods and supports the Pickle protocol.\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"customer_feedback_1.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "doc.to_disk(\"/customer_feedback_1.bin\")\n",
    "#Can't execute below code because permission denied\n",
    "#new_doc = Doc(Vocab()).from_disk(\"/customer_feedback_1.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.5831844\n",
      "pasta <-> hippo 0.120697394\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "#get word vectors and similarity\n",
    "nlp = en_core_web_md.load()\n",
    "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "print(\"apple <-> banana\", apple.similarity(banana))\n",
    "print(\"pasta <-> hippo\", pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
