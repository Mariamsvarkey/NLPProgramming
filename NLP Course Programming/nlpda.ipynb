{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\maria\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from thinc<7.2.0,>=7.1.1->spacy)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba PROPN poss Alibaba NNP Xxxxx True False\n",
      "'s PART case 's POS 'x False True\n",
      "Ant PROPN nmod Ant NNP Xxx True False\n",
      "Financial PROPN npadvmod Financial NNP Xxxxx True False\n",
      "- PUNCT punct - HYPH - False False\n",
      "backed VERB amod back VBN xxxx True False\n",
      "online ADJ amod online JJ xxxx True False\n",
      "food NOUN compound food NN xxxx True False\n",
      "delivery NOUN nmod delivery NN xxxx True False\n",
      "and CCONJ cc and CC xxx True True\n",
      "restaurant NOUN conj restaurant NN xxxx True False\n",
      "discovery NOUN compound discovery NN xxxx True False\n",
      "platform NOUN compound platform NN xxxx True False\n",
      "Zomato PROPN nsubj Zomato NNP Xxxxx True False\n",
      "has AUX aux have VBZ xxx True True\n",
      "acquired VERB ROOT acquire VBN xxxx True False\n",
      "Uber PROPN compound Uber NNP Xxxx True False\n",
      "Eats NOUN dobj eat NNS Xxxx True False\n",
      ", PUNCT punct , , , False False\n",
      "the DET det the DT xxx True True\n",
      "food NOUN compound food NN xxxx True False\n",
      "delivery NOUN compound delivery NN xxxx True False\n",
      "business NOUN appos business NN xxxx True False\n",
      "of ADP prep of IN xx True True\n",
      "ride NOUN npadvmod ride NN xxxx True False\n",
      "- PUNCT punct - HYPH - False False\n",
      "hailing VERB amod hail VBG xxxx True False\n",
      "giant NOUN compound giant NN xxxx True False\n",
      "Uber PROPN compound Uber NNP Xxxx True False\n",
      "India PROPN pobj India NNP Xxxxx True False\n",
      "for ADP prep for IN xxx True True\n",
      "around ADP pobj around IN xxxx True True\n",
      "Rs NOUN quantmod r NNS Xx True False\n",
      "2,485 NUM nummod 2,485 CD d,ddd False False\n",
      "crore NOUN pobj crore NN xxxx True False\n",
      "( PUNCT punct ( -LRB- ( False False\n",
      "$ SYM quantmod $ $ $ False False\n",
      "350 NUM compound 350 CD ddd False False\n",
      "million NUM appos million CD xxxx True False\n",
      ") PUNCT punct ) -RRB- ) False False\n",
      "in ADP prep in IN xx True True\n",
      "an DET det an DT xx True True\n",
      "all DET det all DT xxx True True\n",
      "- PUNCT punct - HYPH - False False\n",
      "stock NOUN compound stock NN xxxx True False\n",
      "deal NOUN pobj deal NN xxxx True False\n",
      ". PUNCT punct . . . False False\n"
     ]
    }
   ],
   "source": [
    "para = nlp(\"Alibaba's Ant Financial-backed online food delivery and restaurant discovery platform Zomato has acquired Uber Eats, the food delivery business of ride-hailing giant Uber India for around Rs 2,485 crore ($350 million) in an all-stock deal.\")\n",
    "for words in para:\n",
    "    print(words.text, words.pos_, words.dep_, words.lemma_, words.tag_, words.shape_, words.is_alpha, words.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba 0 7 GPE\n",
      "Ant Financial 10 23 ORG\n",
      "Zomato 86 92 PERSON\n",
      "Uber Eats 106 115 ORG\n",
      "Uber India 166 176 ORG\n",
      "Rs 2,485 188 196 PRODUCT\n",
      "$350 million 204 216 MONEY\n"
     ]
    }
   ],
   "source": [
    "for entities in para.ents:\n",
    "    print(entities.text, entities.start_char, entities.end_char, entities.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibaba True 22.744986 True\n",
      "'s True 20.978903 True\n",
      "Ant True 21.636627 True\n",
      "Financial True 20.234028 True\n",
      "- True 23.768276 True\n",
      "backed True 22.061193 True\n",
      "online True 21.41796 True\n",
      "food True 21.527428 True\n",
      "delivery True 20.27557 True\n",
      "and True 23.32936 True\n",
      "restaurant True 19.598656 True\n",
      "discovery True 22.446669 True\n",
      "platform True 19.835129 True\n",
      "Zomato True 22.456266 True\n",
      "has True 26.391788 True\n",
      "acquired True 25.842052 True\n",
      "Uber True 22.01978 True\n",
      "Eats True 19.86154 True\n",
      ", True 21.438381 True\n",
      "the True 22.869097 True\n",
      "food True 21.182035 True\n",
      "delivery True 19.951597 True\n",
      "business True 20.1251 True\n",
      "of True 23.678717 True\n",
      "ride True 23.900358 True\n",
      "- True 22.439745 True\n",
      "hailing True 23.02981 True\n",
      "giant True 21.091404 True\n",
      "Uber True 22.620602 True\n",
      "India True 21.643913 True\n",
      "for True 23.218603 True\n",
      "around True 21.182632 True\n",
      "Rs True 21.86625 True\n",
      "2,485 True 23.570608 True\n",
      "crore True 20.267752 True\n",
      "( True 21.124392 True\n",
      "$ True 24.05004 True\n",
      "350 True 25.238487 True\n",
      "million True 25.183613 True\n",
      ") True 22.514511 True\n",
      "in True 21.537016 True\n",
      "an True 22.789968 True\n",
      "all True 23.529083 True\n",
      "- True 25.73238 True\n",
      "stock True 22.724487 True\n",
      "deal True 21.046707 True\n",
      ". True 23.603806 True\n"
     ]
    }
   ],
   "source": [
    "for words in para:\n",
    "    print(words.text, words.has_vector, words.vector_norm, words.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.vocab.strings[\"unique\"])  # 3197928453018144401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP DA due on 31st Jan 2020.', 'Hello.', 'Hi']\n"
     ]
    }
   ],
   "source": [
    "# custom prefix\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "\n",
    "def create_custom_tokenizer(nlp):\n",
    "\n",
    "    prefix_re = re.compile(r'^NLP')\n",
    "    \n",
    "    return Tokenizer(nlp.vocab, prefix_search = prefix_re.search)\n",
    "doc = nlp(r\"NLP DA due on 31st Jan 2020. Hello. Hi\")\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14815333778662080635 subject_nlp 0 1 NLP\n",
      "14815333778662080635 subject_nlp 15 16 nlp\n",
      "17157488710739566268 now 24 25 urgent\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import Matcher\n",
    "m_tool = Matcher(nlp.vocab)\n",
    "p1 = [{'LOWER': 'NLP'}]\n",
    "p2 = [{'LOWER': 'nlp'}]\n",
    "m_tool.add('subject_nlp', None, p1, p2)\n",
    "sentence = nlp(u'NLP DA is due on 31st Jan 2020. Hello, whats up? nlp is cool. Urgent buy biscuits. not urgent')\n",
    "phrase_matches = m_tool(sentence)\n",
    "for match_id, start, end in phrase_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)\n",
    "from spacy.matcher import PhraseMatcher\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "phrases = ['important', 'urgent', 'immediate']\n",
    "patterns = [nlp(text) for text in phrases]\n",
    "phrase_matcher.add('now', None, *patterns)\n",
    "matched_phrases = phrase_matcher(sentence)\n",
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject True 5.62777 False\n",
      "word True 5.8387117 False\n",
      "intelligence True 7.082624 False\n",
      "lalalala True 6.225437 False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(\"subject word intelligence lalalala\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject subject 1.0\n",
      "subject word 0.35317358\n",
      "subject intelligence 0.2781774\n",
      "word subject 0.35317358\n",
      "word word 1.0\n",
      "word intelligence 0.30542892\n",
      "intelligence subject 0.2781774\n",
      "intelligence word 0.30542892\n",
      "intelligence intelligence 1.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "tokens = nlp(\"subject word intelligence\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n",
      "['Ich', 'bin', 'ein', 'Berliner', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, world. Here are two sentences.\")\n",
    "print([t.text for t in doc])\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "doc_de = nlp_de(\"Ich bin ein Berliner.\")\n",
    "print([t.text for t in doc_de])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and updating neural network models\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "train_data = [(\"Alibaba's Ant Financial-backed online food delivery and restaurant discovery platform Zomato has acquired Uber Eats, the food delivery business of ride-hailing giant Uber India for around Rs 2,485 crore ($350 million) in an all-stock deal.\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        for text, annotations in train_data:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_feedback_627.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-bf2a21fd107b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcustomer_feedback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"customer_feedback_627.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustomer_feedback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tmp/customer_feedback_627.bin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customer_feedback_627.txt'"
     ]
    }
   ],
   "source": [
    "#serialization\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "customer_feedback = open(\"customer_feedback_627.txt\").read()\n",
    "doc = nlp(customer_feedback)\n",
    "doc.to_disk(\"/tmp/customer_feedback_627.bin\")\n",
    "\n",
    "new_doc = Doc(Vocab()).from_disk(\"/tmp/customer_feedback_627.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.5831844\n",
      "pasta <-> hippo 0.120697394\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "#get word vectors and similarity\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "#nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print(\"apple <-> banana\", apple.similarity(banana))\n",
    "print(\"pasta <-> hippo\", pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
